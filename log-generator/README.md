## Log Generator

This module will be generating a bulk of random http logs into files, which will be transferred to a HDFS.

### Description:

The **log-generator** will be generating logs in the following format:

- \<ip-address> \<userid> \<timestamp> \<request> \<status-code> \<response-size>
- 192.168.1.15 - [24/Oct/2024:14:05:12] "POST /api/v1/login HTTP/1.1" 401 512

The idea is to simulate large amount of data generated by an HTTP server, which will act as the "big data" to be
processed in the Map-Reduce functions. The size of the generated log files can be configured based on the amount of
generated logs, for local purposes, the files will be smaller, because Hadoop will process them on a single node
sequentially, which may take some time if we to use larger files. After the file has been filled, it will be transferred
to the HDFS, and then removed from the local storage.

### Prerequisites:

- Have a running Hadoop cluster - Name node and at least 1 data node.
- Configure the name node url in the properties file.
- Create a directory where you want the generated log files to be written to locally, and update the properties file.
- Create a directory in the HDFS where you want the files to be transferred to, and update the properties file.

### Expected Behaviour:

- If the directory where you want to generate the local log files doesn't exist, the application will crash.
- If you are trying to write over an existing file in the local directory, the application will crash.
- If the directory where you want to transfer the generate file (in the HDFS) doesn't exist, the application will crash.

### Notes:

Hadoop has a mechanism for splitting the received files or data writes into blocks (default 128MBs), these blocks are
then distributed
across the different data nodes, and the name node keeps track of the metadata about them (where each block is located,
replicated, order, etc...).
The preferred approach for handling the data in Hadoop is to write a large amount of data at once, so the name node can
effectively split it into manageable blocks,
preferably in the ranges of 100+ MBs - 1/2 GBs, but it may vary.
In order to adhere to this approach, the **log-generator** is writing the logs to a local file, it's size will be in the
range of 250-350 MBs,
then it will copy it to the directory on the HDFS.
Rather than streaming small amount of data to the HDFS, which will result in too many blocks, since Hadoop creates a new
block for each write that we make, even if we want to append
to a given file, it will still produce a new block, so if we make 10 000 small writes to a single file, it has to manage
10 000 data blocks which gets cumbersome.
